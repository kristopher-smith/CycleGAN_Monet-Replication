{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GAN's From Scratch \n\nby Kris Smith","metadata":{}},{"cell_type":"markdown","source":"# Problem Statement\n\n***This notebook is working towards generating images in the style of a famous artist painter [Claude Monet](https://en.wikipedia.org/wiki/Claude_Monet). This problem is presented as a competition on the Kaggle platform for the sake of learning. We will explore in particular one common approach to this task in the form of [Generative Adversarial Networks(GANs)](https://en.wikipedia.org/wiki/Generative_adversarial_network).***","metadata":{}},{"cell_type":"code","source":"!pip install albumentations\n\n# import tensorflow as tf\n# from tensorflow import keras\n# from tensorflow.keras import layers\n# import tensorflow_addons as tfa\n\nfrom kaggle_datasets import KaggleDatasets\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nimport os\n# import gdown\nfrom zipfile import ZipFile\n\nimport os\nimport math\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport albumentations as A\n\n# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#     print('Device:', tpu.master())\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# except:\n#     strategy = tf.distribute.get_strategy()\n# print('Number of replicas:', strategy.num_replicas_in_sync)\n\n# AUTOTUNE = tf.data.experimental.AUTOTUNE\n    \n# print(tf.__version__)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Overview\n\n### Dataset Description\n<i>The dataset contains four directories: monet_tfrec, photo_tfrec, monet_jpg, and photo_jpg. The monet_tfrec and monet_jpg directories contain the same painting images, and the photo_tfrec and photo_jpg directories contain the same photos.<i>\n\n<i>We recommend using TFRecords as a Getting Started competition is a great way to become more familiar with a new data format, but JPEG images have also been provided.\n\n<i>The monet directories contain Monet paintings. Use these images to train your model.\n\n<i>The photo directories contain photos. Add Monet-style to these images and submit your generated jpeg images as a zip file. Other photos outside of this dataset can be transformed but keep your submission file limited to 10,000 images.\n\n<i>Note: Monet-style art can be created from scratch using other GAN architectures like DCGAN. The submitted image files do not necessarily have to be transformed photos.\n\n<i>Check out the CycleGAN dataset to experiment with the artistic style of other artists.</i>\n\n    \n### Files\n* monet_jpg - 300 Monet paintings sized 256x256 in JPEG format\n* monet_tfrec - 300 Monet paintings sized 256x256 in TFRecord format\n* photo_jpg - 7028 photos sized 256x256 in JPEG format\n* photo_tfrec - 7028 photos sized 256x256 in TFRecord format\n\n### Submission format\n* Your kernel's output must be called images.zip and contain 7,000-10,000 images sized 256x256.\n    \n    \n### Evaluation\n    \nMiFID\n    \nSubmissions are evaluated on MiFID (Memorization-informed Fr√©chet Inception Distance), which is a modification from Fr√©chet Inception Distance (FID).\n\nThe smaller MiFID is, the better your generated images are.\n\nWhat is FID?\nOriginally published here (github), FID, along with Inception Score (IS), are both commonly used in recent publications as the standard for evaluation methods of GANs.\n\nIn FID, we use the Inception network to extract features from an intermediate layer. Then we model the data distribution for these features using a multivariate Gaussian distribution with mean ¬µ and covariance Œ£. The FID between the real images ùëü\n and generated images ùëî\n is computed as:\n$$\nFID = ||\\mu_{r} - \\mu_{g}||^{2} + Tr(\\Sigma_{r} + \\Sigma_{g} - 2(\\Sigma_{r}\\Sigma_{g})^{1/2})\n$$\n\n    \n    where ùëáùëü sums up all the diagonal elements. FID is calculated by computing the Fr√©chet distance between two Gaussians fitted to feature representations of the Inception network.\n\nWhat is MiFID (Memorization-informed FID)?\nIn addition to FID, Kaggle takes training sample memorization into account.\n\nThe memorization distance is defined as the minimum cosine distance of all training samples in the feature space, averaged across all user generated image samples. This distance is thresholded, and it's assigned to 1.0 if the distance exceeds a pre-defined epsilon.\n\nIn mathematical form:\n    \n$$\nd_{ij} = 1 - \\cos(f_{gi}, f_{rj}) = 1 - \\frac{f_{gi} \\cdot f_{rj}}{||f_{gi}|| ||f_{rj}||}\n$$\n    \nwhere ùëìùëî\n and ùëìùëü\n represent the generated/real images in feature space (defined in pre-trained networks); and ùëìùëîùëñ\n and ùëìùëüùëó\n represent the ùëñùë°‚Ñé\n and ùëóùë°‚Ñé\n vectors of ùëìùëî\n and ùëìùëü\n, respectively.\n\n$$\nd = \\frac{1}{N} \\sum_{i} \\min_{j} d_{ij}\n$$\n    \ndefines the minimum distance of a certain generated image (ùëñ\n) across all real images ((ùëó\n), then averaged across all the generated images.\n\nenter image description here\ndefines the threshold of the weight only applies when the (ùëë\n) is below a certain empirically determined threshold.\n\nFinally, this memorization term is applied to the FID:\n    \n$$\nMiFID = FID * \\frac{1}{d_thr}\n$$\n    \n**Kaggle's workflow calculating MiFID for public and private scores**\n    \nKaggle calculates public MiFID scores with the pre-train neural network Inception, and the public images used for evaluation are the rest of the TFDS Monet paintings. ***Note that as a Getting Started competition there is no private leaderboard.***\n\n    \n### Submission File\n    \nYou are going to generate 7,000-10,000 Monet-style images that are in jpg format. Their sizes should be 256x256x3 (RGB). Then you need to zip those images and your output from your Kernel should only have ONE output file named images.zip.\n\nPlease note that Kaggle Kernels has a number of output files capped at 500. We highly encourage you to either directly write to a zip file as you generate images, or create a folder at ../tmp as your temporary directory.\n","metadata":{}},{"cell_type":"markdown","source":"---\n\n# Data","metadata":{}},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"BASE_PATH = \"../input/gan-getting-started/\"\nMONET_PATH = os.path.join(BASE_PATH, \"monet_jpg\")\nPHOTO_PATH = os.path.join(BASE_PATH, \"photo_jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Image statistics. The following function was provided by the following notebook: https://www.kaggle.com/code/ihelon/monet-visualization-and-augmentation. This function displays the number of images in the monet and photo folders along with their attributes.***","metadata":{}},{"cell_type":"code","source":"def print_folder_statistics(path):\n    d_image_sizes = {}\n    for image_name in os.listdir(path):\n        image = cv2.imread(os.path.join(path, image_name))\n        d_image_sizes[image.shape] = d_image_sizes.get(image.shape, 0) + 1\n        \n    for size, count in d_image_sizes.items():\n        print(f\"shape: {size}\\tcount: {count}\")\n\n\nprint(f\"Monet images:\")\nprint_folder_statistics(MONET_PATH)\nprint(\"-\" * 10)\nprint(f\"Photo images:\")\nprint_folder_statistics(PHOTO_PATH)\nprint(\"-\" * 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***We will focus mainly on the monet images for now. We can see that we are working with 300 of them and each image is 256x256 pixels with 3 channels of color(RGB).***","metadata":{}},{"cell_type":"markdown","source":"### Load Data","metadata":{}},{"cell_type":"code","source":"dataset = keras.utils.image_dataset_from_directory(\n    MONET_PATH, \n    label_mode=None, \n    image_size=(256, 256), \n    batch_size=8\n)\ndataset = dataset.map(lambda x: x / 255.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inspecting Images","metadata":{}},{"cell_type":"code","source":"## Choose the number of images to display\nn = 4\n\n## Create a temporary dataset to stop iteration after the first batch\ndataset_temp = iter(dataset)\n\n## Fetch the first batch\nx = next(dataset_temp)\n\n## Make sure the data is in the correct format\nif isinstance(x, tuple):\n    x = x[0]  \n\nfig, axes = plt.subplots(n, 1, figsize=(20, 20))\n\nfor i in range(n):\n    img = x[i].numpy()\n    img = (img * 255).astype(\"int32\")\n    axes[i].axis(\"off\")\n    axes[i].imshow(img)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"code","source":"\n\n# import tensorflow as tf\n# from tensorflow import keras\n# from tensorflow.keras import layers\n# import tensorflow_addons as tfa\n# import tensorflow_datasets as tfds\n\nfrom kaggle_datasets import KaggleDatasets\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n# from functools import partial\nfrom albumentations import (\n    Compose, RandomBrightness, JpegCompression, HueSaturationValue, RandomContrast, HorizontalFlip,\n    Rotate\n)\n\n# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#     print('Device:', tpu.master())\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# except:\n#     strategy = tf.distribute.get_strategy()\n# print('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n# print(tf.__version__)\n# Load in the data\n# We want to keep our photo dataset and our Monet dataset separate. First, load in the filenames of the TFRecords. We'll load both for the CycleGAN. For the first GAN we only need the Monets as training data.\n\n# All the images for the competition are already sized to 256 x 256. As these images are RGB images, set the channel to 3. Additionally, we need to scale the images to a [-1, 1] scale. Because we are building a generative model, we don't need the labels or the image id so we'll only return the image from the TFRecord.\n\n# GCS_PATH = KaggleDatasets().get_gcs_path()\n\n# MONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\n# print('Monet TFRecord Files:', len(MONET_FILENAMES))\n# You can see I put down a bit of augmentation using random_jitter and flip to increase our data set, because we simply don't have enough data for\n\nIMAGE_SIZE = [256, 256]\n\ndef normalize(image):\n    return (tf.cast(image, tf.float32) / 127.5) - 1\n\ndef decode_image(image):\n    #image = tf.image.decode_jpeg(image, channels=3)\n    #image = tf.reshape(image, [256, 256, 3])\n    image = tf.image.decode_jpeg(image, channels=3)\n    #image = (tf.cast(image, tf.float32) / 127.5) - 1\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\ndef random_crop(image):\n    cropped_image = tf.image.random_crop(image, size=[256, 256, 3])\n    return cropped_image\n\ndef random_jitter(image):\n    # resizing to 286 x 286 x 3 \n    image = tf.image.resize(image, [int(256*1.3), int(256*1.3)],\n                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    # randomly cropping to 256 x 256 x 3\n    image = random_crop(image)\n    # random mirroring\n    return image\n\ndef flip(image):\n    return tf.image.flip_left_right(image)\n\ndef preprocess_image_train(image, label=None):\n    image = random_jitter(image)\n    return image\n\ndef read_tfrecord(example):\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)}\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    return image\n\ndef load_dataset(filenames, labeled=False, ordered=False, repeats=200):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.concatenate(dataset.map(flip, num_parallel_calls=AUTOTUNE).shuffle(100000))\n    dataset = dataset.concatenate(dataset.map(random_jitter, num_parallel_calls=AUTOTUNE).shuffle(100000, reshuffle_each_iteration=True).repeat(repeats))\n    dataset = dataset.map(normalize, num_parallel_calls=AUTOTUNE).shuffle(100000)\n    return dataset\n\n\nmonet_ds = load_dataset(tf.io.gfile.glob(str(KaggleDatasets().get_gcs_path() + '/monet_tfrec/*.tfrec')),\n                        labeled=True, \n                        repeats=150).batch(150, drop_remainder=True)\nmonet_ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create the discriminator\n\nIt maps a 256x256 image to a binary classification score.","metadata":{}},{"cell_type":"code","source":"discriminator = keras.Sequential(\n    [\n        keras.Input(shape=(256, 256, 3)),\n        layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2D(512, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2D(512, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Flatten(),\n        layers.Dropout(0.3),\n        layers.Dense(1, activation=\"sigmoid\"),\n    ],\n    name=\"discriminator\",\n)\ndiscriminator.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create the generator\n\nIt mirrors the discriminator, replacing `Conv2D` layers with `Conv2DTranspose` layers.","metadata":{}},{"cell_type":"code","source":"latent_dim = 512\n\ngenerator = keras.Sequential(\n    [\n        keras.Input(shape=(latent_dim,)),\n        layers.Dense(8 * 8 * 512),\n        layers.Reshape((8, 8, 512)),\n        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        \n        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n    ],\n    name=\"generator\",\n)\ngenerator.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass GAN(keras.Model):\n    def __init__(self, discriminator, generator, latent_dim):\n        super().__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n\n    def compile(self, d_optimizer, g_optimizer, loss_fn):\n        super().compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.loss_fn = loss_fn\n        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n\n    @property\n    def metrics(self):\n        return [self.d_loss_metric, self.g_loss_metric]\n\n    def train_step(self, real_images):\n        # Sample random points in the latent space\n        batch_size = tf.shape(real_images)[0]\n        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n\n        # Decode them to fake images\n        generated_images = self.generator(random_latent_vectors)\n\n        # Combine them with real images\n        combined_images = tf.concat([generated_images, real_images], axis=0)\n\n        # Assemble labels discriminating real from fake images\n        labels = tf.concat(\n            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n        )\n        # Add random noise to the labels - important trick!\n        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n\n        # Train the discriminator\n        with tf.GradientTape() as tape:\n            predictions = self.discriminator(combined_images)\n            d_loss = self.loss_fn(labels, predictions)\n        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n        self.d_optimizer.apply_gradients(\n            zip(grads, self.discriminator.trainable_weights)\n        )\n\n        # Sample random points in the latent space\n        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n\n        # Assemble labels that say \"all real images\"\n        misleading_labels = tf.zeros((batch_size, 1))\n\n        # Train the generator (note that we should *not* update the weights\n        # of the discriminator)!\n        with tf.GradientTape() as tape:\n            predictions = self.discriminator(self.generator(random_latent_vectors))\n            g_loss = self.loss_fn(misleading_labels, predictions)\n        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n\n        # Update metrics\n        self.d_loss_metric.update_state(d_loss)\n        self.g_loss_metric.update_state(g_loss)\n        return {\n            \"d_loss\": self.d_loss_metric.result(),\n            \"g_loss\": self.g_loss_metric.result(),\n        }\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass GANMonitor(keras.callbacks.Callback):\n    def __init__(self, num_img=3, latent_dim=512):\n        self.num_img = num_img\n        self.latent_dim = latent_dim\n\n    def on_epoch_end(self, epoch, logs=None):\n        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n        generated_images = self.model.generator(random_latent_vectors)\n        generated_images *= 255\n        generated_images.numpy()\n        for i in range(self.num_img):\n            img = keras.utils.array_to_img(generated_images[i])\n            img.save(\"generated_img_%03d_%d.png\" % (epoch, i))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 2  # In practice, use ~100 epochs\n\ngan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\ngan.compile(\n    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    loss_fn=keras.losses.BinaryCrossentropy(),\n)\n\ngan.fit(\n    monet_ds, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))\nprint('Photo TFRecord Files:', len(PHOTO_FILENAMES))\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n    return dataset\n\n\nphoto_ds = load_dataset(PHOTO_FILENAMES, labeled=True).batch(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, ax = plt.subplots(5, 2, figsize=(12, 12))\nfor i, img in enumerate(photo_ds.take(5)):\n    prediction = gan.generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n\n    ax[i, 0].imshow(img)\n    ax[i, 1].imshow(prediction)\n    ax[i, 0].set_title(\"Input Photo\")\n    ax[i, 1].set_title(\"Monet-esque\")\n    ax[i, 0].axis(\"off\")\n    ax[i, 1].axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import PIL\n! mkdir ../images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 1\nfor img in photo_ds:\n    prediction = gan.generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    im = PIL.Image.fromarray(prediction)\n    im.save(\"../images/\" + str(i) + \".jpg\")\n    i += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model\ngan.save('gan_model.h5')\ngan.generator.save('gan_generator_model.h5')\ngan.discriminator.save('gan_discriminator_model.h5')\n\n# loaded_gan = keras.models.load_model('gan_model.h5')\n# loaded_generator = keras.models.load_model('gan_generator_model.h5')\n# loaded_discriminator = keras.models.load_model('gan_discriminator_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}